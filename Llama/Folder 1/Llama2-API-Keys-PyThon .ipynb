{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "41a6939d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'replicateAPIKey.txt', 'Use Llama 2 API Keys in PyThon .ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "994dba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "895227bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"REPLICATE_API_TOKEN\"] = open(\"replicateAPIKey.txt\",'r').read().strip(\"\\n\") # Put your API Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "33138a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c699ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter Prompt\n",
    "\n",
    "pre_prompt = \" Hi, you are a helpful assistant. You always respond as a assistant.\"\n",
    "prompt_input = \"What is AI ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "24c448cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LLM Response\n",
    "output = replicate.run(\"a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5\", # LLM Model\n",
    "                       input={\"prompt\": f\"{pre_prompt} {prompt_input} Assistant: \", # Prompts\n",
    "                       \"temperature\":0.1, \"top_p\":0.9, \"max_length\":128, \"repetition_penalty\":1}) # Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "55176033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "def responseFunction(output):\n",
    "    full_response = \" \"\n",
    "    for item in output:\n",
    "        full_response += item\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "28df8d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! I'm LLaMA, I'm based on the LLaMA models (LLaMA: Open and Efficient Foundation Language Models, Touvron et al. 2023). My model is based on the transformer architecture and was trained on a large corpus of text data. I'm here to help answer any questions you have, provide information, and assist with tasks. What would you like to know or discuss?\n"
     ]
    }
   ],
   "source": [
    "response = responseFunction(output)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c177e2cd",
   "metadata": {},
   "source": [
    "                                            -:END:-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
